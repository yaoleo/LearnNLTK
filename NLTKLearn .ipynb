{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokensize http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize\n",
    "# stem http://www.nltk.org/api/nltk.stem.html#module-nltk.stem\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# corpus 数据集\n",
    "from nltk.corpus import stopwords\n",
    "# metrics评分\n",
    "from nltk.metrics import edit_distance\n",
    "# tag\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '!', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '!', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '!', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "['Good muffins cost $3.88\\nin New York!  Please buy me\\ntwo of them.', 'Thanks.']\n"
     ]
    }
   ],
   "source": [
    "s = '''Good muffins cost $3.88\\nin New York!  Please buy me\n",
    "two of them.\\n\\nThanks.'''\n",
    "# 分词\n",
    "print(word_tokenize(s))     # 分词\n",
    "print(regexp_tokenize(s, pattern='\\w+|\\$[\\d\\.]+|\\S+'))# 基于正则的分词\n",
    "print(wordpunct_tokenize(s))# 基于正则的分词 利用空格和标点\n",
    "print(blankline_tokenize(s)) # 基于正则的分词 Uses '\\s*\\n\\s*\\n\\s*'\n",
    "#from nltk.tokenize import BlanklineTokenizer\n",
    "#BlanklineTokenizer().tokenize(s)\n",
    "#from nltk.tokenize import RegexpTokenizer\n",
    "#tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "#tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good muffins cost $3.88\\nin New York!', 'Please buy me\\ntwo of them.', 'Thanks.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '!'],\n",
       " ['Please', 'buy', 'me', 'two', 'of', 'them', '.'],\n",
       " ['Thanks', '.']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 断句\n",
    "all_sent = sent_tokenize(s)\n",
    "print(all_sent)\n",
    "[word_tokenize(t) for t in sent_tokenize(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4),\n",
       " (5, 12),\n",
       " (13, 17),\n",
       " (18, 23),\n",
       " (24, 26),\n",
       " (27, 30),\n",
       " (31, 36),\n",
       " (38, 44),\n",
       " (45, 48),\n",
       " (49, 51),\n",
       " (52, 55),\n",
       " (56, 58),\n",
       " (59, 64),\n",
       " (66, 73)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK tokenizers can produce token-spans, \n",
    "# represented as tuples of integers having the same semantics as string slices, \n",
    "# to support efficient comparison of tokenizers.\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "list(WhitespaceTokenizer().span_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "at\n",
      "shop\n",
      "ate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ate'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词干提取(stemming)\n",
    "pst = PorterStemmer()   #波特干扰算法\n",
    "lst = LancasterStemmer()#基于兰开斯特（Paice / Husk）干扰算法的词干\n",
    "print(lst.stem(\"eating\"))\n",
    "print(lst.stem(\"ate\"))\n",
    "print(pst.stem(\"shopping\"))\n",
    "print(pst.stem(\"ate\"))\n",
    "# 词形还原(lemmatization)，词根(lemma)\n",
    "wlem = WordNetLemmatizer()\n",
    "print(wlem.lemmatize(\"ate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ate'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词形还原(lemmatization)，词根(lemma)\n",
    "wlem = WordNetLemmatizer()\n",
    "wlem.lemmatize(\"ate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'test']\n"
     ]
    }
   ],
   "source": [
    "# 停用词移除(Stop word removal)\n",
    "stoplist = stopwords.words('english')\n",
    "text = \"This is just a test\"\n",
    "cleanwordlist = [word for word in text.split() if word not in stoplist]\n",
    "print(cleanwordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport nltk\\ntoken = text.split()\\nfreq_dist = nltk.FreqDist(token)\\nrarewords = freq_dist.keys()[-50:]\\nafter_rare_words = [word for word in token not in rarewords]\\nprint(after_rare_words)\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 罕见词移除\n",
    "'''\n",
    "import nltk\n",
    "token = text.split()\n",
    "freq_dist = nltk.FreqDist(token)\n",
    "rarewords = freq_dist.keys()[-50:]\n",
    "after_rare_words = [word for word in token not in rarewords]\n",
    "print(after_rare_words)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# 评估\n",
    "from nltk.metrics import edit_distance\n",
    "print(edit_distance(\"rain\", \"shine\")) # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Good', 'JJ'), ('muffins', 'NNS'), ('cost', 'VBP'), ('$', '$'), ('3.88', 'CD'), ('in', 'IN'), ('New', 'NNP'), ('York', 'NNP'), ('!', '.'), ('Please', 'NNP'), ('buy', 'VB'), ('me', 'PRP'), ('two', 'CD'), ('of', 'IN'), ('them', 'PRP'), ('.', '.'), ('Thanks', 'NNS'), ('.', '.')]\n",
      "[('Good', 'JJ'), ('muffins', 'NNS'), ('cost', 'VBP'), ('$', '$'), ('3.88', 'CD'), ('in', 'IN'), ('New', 'NNP'), ('York', 'NNP'), ('!', '.'), ('Please', 'NNP'), ('buy', 'VB'), ('me', 'PRP'), ('two', 'CD'), ('of', 'IN'), ('them', 'PRP'), ('.', '.'), ('Thanks', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 词性标注\n",
    "# 词性(POS)\n",
    "# PennTreebank\n",
    "print(nltk.pos_tag(word_tokenize(s)))\n",
    "print(pos_tag(word_tokenize(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tanford标注器\n",
    "# \n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "#stan_tagger = StanfordPOSTagger('D:/nltk_data/stanford-postagger-full-2017-06-09/models/english-bidirectional-distsim.tagger',\n",
    "#                                'D:/nltk_data/stanford-postagger-full-2017-06-09/stanford-postagger.jar')\n",
    "#s = \"I was watching TV\"\n",
    "#tokens = nltk.word_tokenize(s)\n",
    "#stan_tagger.tag(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 218 samples and 100554 outcomes>\n",
      "0.13089484257215028\n"
     ]
    }
   ],
   "source": [
    "# 深入了解标注器\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "tags = [tag for (word, tag) in brown.tagged_words(categories = 'news')]\n",
    "print(nltk.FreqDist(tags))\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(categories = 'news')\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "print(default_tagger.evaluate(brown_tagged_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "(The, AT), \n",
      "(Fulton, NP-TL), \n",
      "(County, NN-TL), \n",
      "(Grand, JJ-TL), \n",
      "(Jury, NN-TL), \n",
      "(said, VBD), \n",
      "(Friday, NR), \n",
      "(an, AT), \n",
      "(investigation, NN), \n",
      "(of, IN), \n",
      "(Atlanta's, NP$), \n",
      "(recent, JJ), \n",
      "(primary, NN), \n",
      "(election, NN), \n",
      "(produced, VBD), \n",
      "(``, ``), \n",
      "(no, AT), \n",
      "(evidence, NN), \n",
      "('', ''), \n",
      "(that, CS), \n",
      "(any, DTI), \n",
      "(irregularities, NNS), \n",
      "(took, VBD), \n",
      "(place, NN), \n",
      "(., .), \n"
     ]
    }
   ],
   "source": [
    "# 1 N-Gram标注器\n",
    "from nltk.tag import UnigramTagger# 在训练语料库中找到每个单词的最可能的标签，然后使用该信息为新的标记分配标签。\n",
    "test_sent = brown.sents(categories='news')[0]\n",
    "print(test_sent)\n",
    "unigram_tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])\n",
    "for tok, tag in unigram_tagger.tag(test_sent):\n",
    "    print(\"(%s, %s), \" % (tok, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'NN'), ('is', 'NN'), ('a', 'NN'), ('test', 'NN')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger#为每个token分配相同标记的标记器\n",
    "default_tagger = DefaultTagger('NN')\n",
    "list(default_tagger.tag('This is a test'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8364397488288647\n",
      "0.84570915977275\n",
      "0.84411442240606\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import BigramTagger  # 根据前一个的标签\n",
    "from nltk.tag import TrigramTagger # 根据前两个的标签\n",
    "train_data = brown_tagged_sents[:int(len(brown_tagged_sents) * 0.9)]\n",
    "test_data = brown_tagged_sents[int(len(brown_tagged_sents) * 0.9):]\n",
    "unigram_tagger = UnigramTagger(train_data, backoff=default_tagger)\n",
    "print(unigram_tagger.evaluate(test_data)) # 根据标准评分标签的准确性。\n",
    "biggram_tagger = BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print(biggram_tagger.evaluate(test_data))\n",
    "trigram_tagger = TrigramTagger(train_data, backoff=biggram_tagger)\n",
    "print(trigram_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31306687929831556\n"
     ]
    }
   ],
   "source": [
    "# 2 正则表达式标注器\n",
    "from nltk.tag.sequential import RegexpTagger\n",
    "regexp_tagger = RegexpTagger(\n",
    "    [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "     (r'(The|the|A|a|An|an)$', 'AT'),  # articles\n",
    "     (r'.*able$', 'JJ'), # adjectives\n",
    "     (r'.*ness$', 'NN'), # nouns formed from adj\n",
    "     (r'.*ly$', 'RB'),   # adverbs\n",
    "     (r'.*s$', 'NNS'),   # plural nouns\n",
    "     (r'.*ing$', 'VBG'), # gerunds\n",
    "     (r'.*ed$', 'VBD'),  # past tense verbs\n",
    "     (r'.*', 'NN')       # nouns (default)\n",
    "    ])\n",
    "print(regexp_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.1.4 Brill 标注器\n",
    "# 3.1.5 基于机器学习的标注器\n",
    "# 最大熵分类器(MEC)\n",
    "# 隐性马尔科夫模型(HMM)\n",
    "# 条件随机场(CRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('very', 'RB'), ('excited', 'JJ'), ('about', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('generation', 'NN'), ('of', 'IN'), ('Apple', 'NNP'), ('products', 'NNS'), ('.', '.')]\n",
      "(S\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  very/RB\n",
      "  excited/JJ\n",
      "  about/IN\n",
      "  the/DT\n",
      "  next/JJ\n",
      "  generation/NN\n",
      "  of/IN\n",
      "  (GPE Apple/NNP)\n",
      "  products/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# 命名实体识别(NER)\n",
    "# NER标注器\n",
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "tokens = nltk.word_tokenize('I am very excited about the next generation of Apple products.')\n",
    "tokens = nltk.pos_tag(tokens)\n",
    "print (tokens)\n",
    "tree = nltk.ne_chunk(tokens)\n",
    "print (tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "#from nltk.tag.stanford import StanfordNERTagger\n",
    "# https://nlp.stanford.edu/software/stanford-ner-2017-06-09.zip\n",
    "#st = StanfordNERTagger('D:/nltk_data/stanford-ner-2017-06-09/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                       'D:/nltk_data/stanford-ner-2017-06-09/stanford-ner.jar')\n",
    "#st.tag('Rami Eid is studying at Stony Brook University in NY'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
