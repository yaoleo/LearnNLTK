{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# tokensize http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize\n",
    "# stem http://www.nltk.org/api/nltk.stem.html#module-nltk.stem\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# corpus 数据集\n",
    "from nltk.corpus import stopwords\n",
    "# metrics评分\n",
    "from nltk.metrics import edit_distance\n",
    "# tag\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 命名实体识别\n",
    "# 分句-分词-词性标签-命名实体识别-实体关系识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document = open(\"test.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_InformationExtract(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "test_InformationExtract(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "# 实体识别的基本技术是分块（chunking chunking chunking chunking）\n",
    "# 基于正则的分块\n",
    "# 名词短语分块，或 NP-分块（NP-chunking NP-chunking NP-chunking NP-chunking），寻找单独名词短语对应的块。\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),(\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\" \n",
    "cp = nltk.RegexpParser(grammar) \n",
    "result = cp.parse(sentence)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Rapunzel/NNP)\n",
      "  let/VBD\n",
      "  down/RP\n",
      "  (NP her/PP$ long/JJ golden/JJ hair/NN))\n"
     ]
    }
   ],
   "source": [
    "# 要找到一个给定的句子的块结构，RegexpParser 分块器以一个没有标识符被分块的平面结构开始。\n",
    "# 轮流应用分块规则，依次更新块结构。所有的规则都被调用后，返回块结构。\n",
    "grammar = r\"\"\"\n",
    "NP: {<DT|PP\\$>?<JJ>*<NN>} # chunk determiner/possessive, adjectives and nouns\n",
    "{<NNP>+} # chunk sequences of proper nouns\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"),\n",
    "(\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]\n",
    "print (cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PP Over/IN)\n",
      "  (NP a/DT cup/NN)\n",
      "  (PP of/IN)\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  (VP told/VBD)\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  Over/IN\n",
      "  (NP a/DT cup/NN)\n",
      "  of/IN\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  told/VBD\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# 读取分块语料库\n",
    "from nltk.corpus import conll2000\n",
    "print (conll2000.chunked_sents('train.txt')[99])\n",
    "# NP块是名词短语 VP 块如 has already delivered；PP 块如 because of\n",
    "# 如果只想要NP块\n",
    "print (conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99])\n",
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n有时词性标记不足以确定一个句子应如何分块。例如：考虑下面的两个语句：\\n\\na. Joey/NN sold/VBD the/DT farmer/NN rice/NN ./.\\nb. Nick/NN broke/VBD my/DT computer/NN monitor/NN ./.\\n\\n这两句话的词性标记相同，但分块方式不同。在第一句中，the farmer 和 rice 都是单独\\n的块，而在第二个句子中相应的部分，the computer monitor，是一个单独的块。显然，如\\n果我们想最大限度地提升分块的性能，我们需要使用词的内容信息作为词性标记的补充。\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练基于分类器的分块器\n",
    "\"\"\"\n",
    "有时词性标记不足以确定一个句子应如何分块。例如：考虑下面的两个语句：\n",
    "\n",
    "a. Joey/NN sold/VBD the/DT farmer/NN rice/NN ./.\n",
    "b. Nick/NN broke/VBD my/DT computer/NN monitor/NN ./.\n",
    "\n",
    "这两句话的词性标记相同，但分块方式不同。在第一句中，the farmer 和 rice 都是单独\n",
    "的块，而在第二个句子中相应的部分，the computer monitor，是一个单独的块。显然，如\n",
    "果我们想最大限度地提升分块的性能，我们需要使用词的内容信息作为词性标记的补充。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 级联分块器构建嵌套结构\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
